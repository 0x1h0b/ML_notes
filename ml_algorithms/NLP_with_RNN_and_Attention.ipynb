{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446e1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cc5d17",
   "metadata": {},
   "source": [
    "### Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1d19a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://homl.info/shakespeare\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\",url)\n",
    "with open(filepath) as fp:\n",
    "    shakes_text = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd55c7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakes_text[:100] # first 100 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe944d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode every character as integer \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d9cbfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 6, 9, 8, 3]]\n",
      "['f i r s t']\n"
     ]
    }
   ],
   "source": [
    "# testing tokenizer\n",
    "x = tokenizer.texts_to_sequences(['First'])\n",
    "print(x)\n",
    "x = tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "923fcfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_id: 39\n"
     ]
    }
   ],
   "source": [
    "# number of distict chars\n",
    "max_id = len(tokenizer.word_index)\n",
    "print('max_id:',max_id)\n",
    "# tokenizer.word_index\n",
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4981153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_words': None,\n",
       " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
       " 'lower': True,\n",
       " 'split': ' ',\n",
       " 'char_level': True,\n",
       " 'oov_token': None,\n",
       " 'document_count': 1115394,\n",
       " 'word_counts': '{\"f\": 17567, \"i\": 57369, \"r\": 53758, \"s\": 54219, \"t\": 74024, \" \": 169892, \"c\": 19443, \"z\": 554, \"e\": 100652, \"n\": 53608, \":\": 10316, \"\\\\n\": 40000, \"b\": 14082, \"o\": 71279, \"w\": 21115, \"p\": 12449, \"d\": 33447, \"a\": 63326, \"y\": 22166, \"u\": 29897, \"h\": 54378, \",\": 19846, \"m\": 25083, \"k\": 8672, \".\": 7885, \"l\": 37215, \"v\": 8591, \"?\": 2462, \"\\'\": 6187, \"g\": 15755, \";\": 3628, \"!\": 2172, \"j\": 948, \"-\": 1897, \"q\": 840, \"x\": 641, \"&\": 3, \"3\": 27, \"$\": 1}',\n",
       " 'word_docs': '{\"f\": 17567, \"i\": 57369, \"r\": 53758, \"s\": 54219, \"t\": 74024, \" \": 169892, \"c\": 19443, \"z\": 554, \"e\": 100652, \"n\": 53608, \":\": 10316, \"\\\\n\": 40000, \"b\": 14082, \"o\": 71279, \"w\": 21115, \"p\": 12449, \"d\": 33447, \"a\": 63326, \"y\": 22166, \"u\": 29897, \"h\": 54378, \",\": 19846, \"m\": 25083, \"k\": 8672, \".\": 7885, \"l\": 37215, \"v\": 8591, \"?\": 2462, \"\\'\": 6187, \"g\": 15755, \";\": 3628, \"!\": 2172, \"j\": 948, \"-\": 1897, \"q\": 840, \"x\": 641, \"&\": 3, \"3\": 27, \"$\": 1}',\n",
       " 'index_docs': '{\"20\": 17567, \"6\": 57369, \"9\": 53758, \"8\": 54219, \"3\": 74024, \"1\": 169892, \"19\": 19443, \"36\": 554, \"2\": 100652, \"10\": 53608, \"24\": 10316, \"11\": 40000, \"22\": 14082, \"4\": 71279, \"17\": 21115, \"23\": 12449, \"13\": 33447, \"5\": 63326, \"16\": 22166, \"14\": 29897, \"7\": 54378, \"18\": 19846, \"15\": 25083, \"25\": 8672, \"27\": 7885, \"12\": 37215, \"26\": 8591, \"30\": 2462, \"28\": 6187, \"21\": 15755, \"29\": 3628, \"31\": 2172, \"33\": 948, \"32\": 1897, \"34\": 840, \"35\": 641, \"38\": 3, \"37\": 27, \"39\": 1}',\n",
       " 'index_word': '{\"1\": \" \", \"2\": \"e\", \"3\": \"t\", \"4\": \"o\", \"5\": \"a\", \"6\": \"i\", \"7\": \"h\", \"8\": \"s\", \"9\": \"r\", \"10\": \"n\", \"11\": \"\\\\n\", \"12\": \"l\", \"13\": \"d\", \"14\": \"u\", \"15\": \"m\", \"16\": \"y\", \"17\": \"w\", \"18\": \",\", \"19\": \"c\", \"20\": \"f\", \"21\": \"g\", \"22\": \"b\", \"23\": \"p\", \"24\": \":\", \"25\": \"k\", \"26\": \"v\", \"27\": \".\", \"28\": \"\\'\", \"29\": \";\", \"30\": \"?\", \"31\": \"!\", \"32\": \"-\", \"33\": \"j\", \"34\": \"q\", \"35\": \"x\", \"36\": \"z\", \"37\": \"3\", \"38\": \"&\", \"39\": \"$\"}',\n",
       " 'word_index': '{\" \": 1, \"e\": 2, \"t\": 3, \"o\": 4, \"a\": 5, \"i\": 6, \"h\": 7, \"s\": 8, \"r\": 9, \"n\": 10, \"\\\\n\": 11, \"l\": 12, \"d\": 13, \"u\": 14, \"m\": 15, \"y\": 16, \"w\": 17, \",\": 18, \"c\": 19, \"f\": 20, \"g\": 21, \"b\": 22, \"p\": 23, \":\": 24, \"k\": 25, \"v\": 26, \".\": 27, \"\\'\": 28, \";\": 29, \"?\": 30, \"!\": 31, \"-\": 32, \"j\": 33, \"q\": 34, \"x\": 35, \"z\": 36, \"3\": 37, \"&\": 38, \"$\": 39}'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.document_count\n",
    "tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a21c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d3838d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual ids are from 1 to 39 , by substracting -1 we will get in between 0 to 1\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakes_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e68b2",
   "metadata": {},
   "source": [
    "- spliting dataset into train and test set  as it is sequential data we can not shuffle it and make sure to not have overlapping content between different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27d10dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  1003854\n"
     ]
    }
   ],
   "source": [
    "train_size = total_chars*90 // 100\n",
    "print(\"train size: \",train_size)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c259a",
   "metadata": {},
   "source": [
    "##### Chopping the Sequential dataset into multiple windows\n",
    "- if we train our current 1 million inputs (chars ) to go through RNN ,it will iterate 1M time which is equivalent to 1M parameters.\n",
    "- so we use datasets Window() method to convert this long sentence of characters into smaller windows of text.\n",
    "- Every instance in the dataset will be fairly short substring of the whole text, and RNN will be unrolled only over the length of these substrings, this is called truncated backpropagation through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a105009",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length =  n_steps + 1\n",
    "dataset = dataset.window(window_length,shift=1,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "704c2be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: (None,), types: tf.int64>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebe32ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None, None), types: tf.int64>\n",
      "<MapDataset shapes: ((None, None), (None, None)), types: (tf.int64, tf.int64)>\n",
      "<MapDataset shapes: ((None, None, 39), (None, None)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "print(dataset)\n",
    "dataset = dataset.map(lambda windows: (windows[:,:-1],windows[:,1:]))\n",
    "print(dataset)\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "print(dataset)\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21d95eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3fd32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.GRU(128,return_sequences=True,input_shape=[None,max_id],dropout=0.2,recurrent_dropout=0.2),\n",
    "#     tf.keras.layers.GRU(128,return_sequences=True,dropout=0.2,recurrent_dropout=0.2),\n",
    "#     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_id,activation=\"softmax\"))\n",
    "# ])\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\")\n",
    "# history = model.fit(dataset,epochs=20)\n",
    "\n",
    "# takes too much time -> to train but code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6385c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    x = np.array(tokenizer.texts_to_sequences(text))\n",
    "    print(x)\n",
    "    return tf.one_hot(x,max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e7dea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  4 17  1  5  9  2  1 16  4 14 30]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(39,), dtype=float32, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text([\"how are you?\"])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "676e0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "#Y_pred = model.predict_classes(X_new)\n",
    "# Y_pred = np.argmax(model(X_new)a, axis=-1)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7694c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09318f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959ddaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5883a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
