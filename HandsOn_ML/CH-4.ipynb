{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b229601a",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "- This chapter will cover mostly Linear Regression in depth\n",
    "\n",
    "\n",
    "#### Linear Regression\n",
    "- vectorized equation :- `y = Theta . X`\n",
    "    - theta is the model's parameter vector , containing the bias\n",
    "- MSE (Mean Squared Error)\n",
    "  - Mean of summation of squared error sum((thetaT.xi - yi)**2)/m.\n",
    "- Direct method of calculating model parameters (matrix manipulation)\n",
    "    - time complexity increases with no of features by x4.\n",
    "    \n",
    "#### Gradient Descent\n",
    "- Gradient Descent is a very genereic optimization algorithm capable of finding optimal solutions to a wide range of problems .\n",
    "    - the general idea is to tweak parameters iteratively in order to optimize a cost function.\n",
    "    - Concretely, you start by filling θ with random values (this is called random initializa‐ tion), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum\n",
    "    - another important parameter is the learning rate (size of steps gradient descent takes to tweak the parametrs)\n",
    "        - if it is too small then the algorithm will have to go through many iterations to converge , which will take long time.\n",
    "        - if its too high , you might jump across the minimum value and end up on the other side (Possibly even higher then before).\n",
    "    - not all gradient descent function is in convex , there might be lots of local minumum points with a single global minimum.\n",
    "        - MSE function is a convex function so there just one minima /maxima.\n",
    "    - the cost function has the shape of a bowl, but it can be an elongated bowl if the features have very different scales.Gradient Descent on a training set where features 1 and 2 have the same scale (on the left), and on a training set where feature 1 has much smaller values than feature 2.\n",
    "- To implement Gradient Descent, you need to compute the gradient of the cost function with regards to each model parameter θj.\n",
    "    - you need to calculate how much the cost function will change if you change θj just a little bit. This is called a partial derivative. \n",
    "    - It is like asking “what is the slope of the mountain under my feet if I face east?” and then asking the same question facing north (and so on for all other dimensions, if you can imagine a universe with more than three dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3388a7",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b852f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
